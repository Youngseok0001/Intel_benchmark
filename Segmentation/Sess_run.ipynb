{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from attention_unet import *\n",
    "from deeplab_model import *\n",
    "from data_pipeline import *\n",
    "import numpy as np\n",
    "import os as os\n",
    "\n",
    "# Select single gpu\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 1.0\n",
    "sess=tf.Session(config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "tf.reset_default_graph()\n",
    "lr =  2e-4\n",
    "model = attention_gating_unet2D(num_classes=1)\n",
    "# TRAINING MODEL\n",
    "# forward propagation\n",
    "def forward(model, data_generator):\n",
    "    \n",
    "    images = data_generator[0]\n",
    "    labels = data_generator[1]\n",
    "    logits = model(images) # computes logits with variables\n",
    "    \n",
    "    return logits, labels\n",
    "\n",
    "def compute_loss(model, data_generator, loss_fn='default'):\n",
    "    \n",
    "    logits, labels = forward(model,data_generator)\n",
    "    if loss_fn == 'default':\n",
    "        loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "    losses = loss_fn(labels, logits)\n",
    "    \n",
    "    return tf.reduce_mean(losses), logits, labels\n",
    "    \n",
    "\n",
    "\n",
    "def start_training(sess, num_epoch, loss, train_logit, train_label, train_op, train_acc,\n",
    "                   test_logit, test_label, test_acc, batch_size):\n",
    "       \n",
    "    variables = (train_acc.variables + test_acc.variables)\n",
    "    # inits\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run([v.initializer for v in variables])\n",
    "    \n",
    "    # MAKE TIME LIST\n",
    "    train_times = []\n",
    "    test_times = []\n",
    "\n",
    "    for e in range(num_epoch):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        num_steps = len(imgs_train)// batch_size\n",
    "        for i in tqdm(range(num_steps)):\n",
    "            \n",
    "            loss_value, logits, labels, _  = sess.run([loss, train_logit, train_label, train_op])\n",
    "            train_acc(labels, logits)\n",
    "            \n",
    "            if i % 20 == 0:\n",
    "                print(\"--------------------------- Current epoch: {} --------------------------- \".format(e))\n",
    "                print(\"Training loss at {}th iteration is {}\".format(i, loss_value))\n",
    "\n",
    "        end_time = time.time()\n",
    "        time_diff = end_time - start_time\n",
    "        train_times.append(time_diff)\n",
    "        \n",
    "        train_acc_value = sess.run(train_acc.result())\n",
    "        print(\"Training binary accuracy for last epoch: {}\".format(train_acc_value))\n",
    "        train_acc.reset_states()\n",
    "        print(\"Time taken for last training epoch is {}s\".format(time_diff))\n",
    "        print(\"Avg time for last training epoch is {}s\".format(time_diff/num_steps))\n",
    "                                \n",
    "        \n",
    "        # VALIDATION STEPS\n",
    "        start_test_time = time.time()\n",
    "        \n",
    "        num_steps_test = len(imgs_test)//batch_size\n",
    "        for i in range(num_steps_test):\n",
    "            logits, labels = sess.run([test_logit, test_label])\n",
    "            test_acc(labels, logits)\n",
    "            \n",
    "        end_test_time = time.time()\n",
    "        test_time_diff = end_test_time - start_test_time\n",
    "        test_times.append(test_time_diff)\n",
    "        \n",
    "        print(\"--------------------------- Validation Step --------------------------- \")\n",
    "        print(\"Time taken for last validation epoch is {}s\".format(test_time_diff))\n",
    "        print(\"Avg time for last validation epoch is {}s\".format(test_time_diff/num_steps_test))\n",
    "        valid_acc_value = sess.run(test_acc.result())\n",
    "        print(\"Mean validation binary accuracy for last epoch: {}\".format(valid_acc_value))\n",
    "        valid_acc_value.reset_states()\n",
    "               \n",
    "    mean_train_time = np.mean(train_times)\n",
    "    max_train_time = np.max(train_times)\n",
    "    min_train_time = np.min(train_times)\n",
    "    print(\"Time taken for complete training cycle: mean: {}[{},{}] /epoch\".format(mean_train_time,\n",
    "                                                                   min_train_time,\n",
    "                                                                   max_train_time))\n",
    "    \n",
    "    mean_test_time = np.mean(test_times)\n",
    "    max_test_time = np.max(test_times)\n",
    "    min_test_time = np.min(test_times)\n",
    "    \n",
    "    print(\"Time for complete validations: mean: {}[{},{}] /epoch\".format(mean_test_time,\n",
    "                                                                   min_test_time,\n",
    "                                                                   max_test_time))\n",
    "    \n",
    "\n",
    "def train_model(model, batch_size, lr):\n",
    "    \n",
    "    # ESTABLISH DATA PIPELINE\n",
    "    # SPLIT: PATH DIR -DATA(TRAIN/TEST)[0.8/0.2]\n",
    "    imgs_train, label_train, imgs_test, label_test = split_datalist(train_test_ratio=0.8)\n",
    "    \n",
    "    # MAKE GENERATOR\n",
    "    train_generator = input_fn(imgs_train, label_train, batch_size,num_parallel_calls= 18).make_one_shot_iterator()\n",
    "    test_generator = input_fn(imgs_test, label_test, batch_size, num_parallel_calls= 18).make_one_shot_iterator()\n",
    "    \n",
    "    def input_train_fn():\n",
    "        with tf.device(None):\n",
    "            return train_generator.get_next()\n",
    "        \n",
    "    # SET OPTIMIZER\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "    \n",
    "    # EVALUATING METRICS\n",
    "    train_acc_metric = tf.keras.metrics.BinaryAccuracy()\n",
    "    valid_acc_metric = tf.keras.metrics.BinaryAccuracy()\n",
    "    \n",
    "    # COMPUTE LOSS\n",
    "    train_loss, train_logits, train_labels = compute_loss(model, input_train_fn())\n",
    "    \n",
    "    grads = optimizer.compute_gradients(train_loss)\n",
    "    apply_gradient_op = optimizer.apply_gradients(grads)\n",
    "    \n",
    "    # DEFINE TEST VARIABLES\n",
    "    logits_test, labels_test = forward(model, test_generator.get_next())\n",
    "    \n",
    "    with tf.Session() as sess:        \n",
    "        start_training(sess, 15, train_loss, train_logits, train_labels, apply_gradient_op, train_acc_metric,\n",
    "                   logits_test, labels_test, valid_acc_metric, batch_size)\n",
    "        \n",
    "    \n",
    "train_model(model, 15,lr) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
